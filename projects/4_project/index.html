<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Case Study | Sina Torfi </title> <meta name="author" content="Sina Torfi"> <meta name="description" content="Advanced Cross-Modal Knowledge Transfer for Enhanced Visual Question Answering and Image Captioning Using CLIP and Vision Transformers."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://astorfi.github.io/projects/4_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sina</span> Torfi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Start Here </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Case Study</h1> <p class="post-description">Advanced Cross-Modal Knowledge Transfer for Enhanced Visual Question Answering and Image Captioning Using CLIP and Vision Transformers.</p> </header> <article> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/multi_modal_clip-480.webp 480w,/assets/img/projects/multi_modal_clip-800.webp 800w,/assets/img/projects/multi_modal_clip-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/multi_modal_clip.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Advanced Cross-Modal Knowledge Transfer" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Advanced Cross-Modal Knowledge Transfer. </div> <hr> <h4 id="1-summary"><strong>1. Summary</strong></h4> <p>This project tackled a key challenge in multimodal AI: bridging the interpretative gap between text and visual data in applications like Visual Question Answering (VQA) and Image Captioning. Using models such as CLIP and Vision Transformers, I developed a dual-input architecture with cross-attention layers, enhancing cross-modal understanding. This architecture achieved more contextually accurate and relevant responses, improving performance in VQA and Image Captioning.</p> <hr> <h4 id="2-questions-this-case-study-answers"><strong>2. Questions This Case Study Answers</strong></h4> <p><strong>Project Overview and Problem Understanding</strong></p> <ul> <li>What specific challenge in multimodal AI did this project aim to solve?</li> <li>Why is connecting textual and visual data important for VQA and Image Captioning?</li> </ul> <p><strong>Technical Approach and Architecture</strong></p> <ul> <li>What was the dual-input architecture used for VQA and Image Captioning?</li> <li>Why integrate CLIP and Vision Transformers?</li> <li>How do cross-attention mechanisms improve the model’s cross-modal performance?</li> <li>What benefits do Vision Transformers provide over CNNs for multimodal tasks?</li> </ul> <p><strong>Optimization Techniques</strong></p> <ul> <li>How was CLIP fine-tuned for better cross-modal understanding?</li> <li>Which semi-supervised techniques addressed data scarcity, and how did they improve performance?</li> </ul> <p><strong>Data Handling and Augmentation</strong></p> <ul> <li>Which datasets were chosen, and why?</li> <li>How did pseudo-labeling and self-training enhance the model?</li> <li>How did masked language modeling strengthen multimodal comprehension?</li> </ul> <p><strong>Implementation and Tools</strong></p> <ul> <li>What tools and frameworks supported CLIP and Vision Transformers?</li> <li>How were cross-attention mechanisms integrated for cross-modal interactions?</li> <li>What role did distributed training play in optimizing model performance?</li> </ul> <p><strong>Results and Impact</strong></p> <ul> <li>What improvements were achieved in VQA accuracy over baseline models?</li> <li>How did the model perform in Image Captioning in terms of BLEU and METEOR scores?</li> <li>What feedback did users give about the system’s responses?</li> <li>How does the model’s ability to handle detailed visual contexts enhance user satisfaction?</li> </ul> <p><strong>Skills and Learning</strong></p> <ul> <li>Which techniques were most effective in cross-modal knowledge transfer?</li> <li>What challenges arose in integrating CLIP with Vision Transformers, and how were they solved?</li> <li>How did experience with generative AI and cross-attention help in this project?</li> </ul> <hr> <h4 id="3-problem"><strong>3. Problem</strong></h4> <p>Multimodal AI systems, particularly those used for Visual Question Answering (VQA) and Image Captioning, often struggle to connect information across text and images. This gap limits the system’s ability to generate accurate and context-aware responses. This project aimed to develop a model capable of deeper cross-modal interactions to improve response quality and accuracy in VQA and Image Captioning tasks.</p> <hr> <h4 id="4-significance"><strong>4. Significance</strong></h4> <p>Effective integration of text and image data is critical for applications that need a nuanced understanding across modalities, such as assistive technologies, media, and content creation. Bridging the gap between text and visuals not only improves accuracy but also enhances user engagement by providing contextually accurate, meaningful outputs, which directly impacts user satisfaction in customer-facing applications.</p> <hr> <h4 id="5-solution-approach"><strong>5. Solution Approach</strong></h4> <p>To address this challenge, I developed a dual-input architecture combining multimodal models and cross-attention mechanisms, with CLIP and Vision Transformers at its core.</p> <p><strong>Core Architecture: CLIP and Vision Transformers</strong> At the heart of the architecture is CLIP, a model that learns joint visual-text embeddings, allowing seamless text-image alignment. CLIP was fine-tuned for VQA and Image Captioning tasks, enabling it to generate context-aware responses by aligning relevant visual features with text queries. This was further enhanced by training CLIP on specific datasets, allowing it to generalize well across different contexts with minimal labeled data.</p> <p>To improve visual data processing, I used Vision Transformers (ViTs) instead of traditional CNNs. Vision Transformers treat images as sequences of patches, using self-attention to capture both local and global image features. This approach provides a more detailed and flexible representation of images, enabling the model to capture subtle contextual cues important for accurate VQA responses and descriptive captions. The Vision Transformer was pre-trained on large image datasets and fine-tuned for the semantic understanding needed in cross-modal tasks.</p> <p><strong>Dual-Input and Cross-Attention Mechanisms</strong> The architecture uses separate pathways for visual and textual data, with cross-attention layers enabling the two modalities to interact. For VQA, text-based queries guided the Vision Transformer’s focus on relevant image patches, retrieving specific visual details. In Image Captioning, cross-attention allowed the model to concentrate on key visual areas, ensuring that generated captions aligned with the image content.</p> <p><strong>Data Handling and Augmentation</strong> The model was trained on task-specific datasets like MS COCO for Image Captioning and VQA v2.0, which helped it learn dependencies between text and image data. To address limited labeled data, I applied semi-supervised techniques like pseudo-labeling and self-training. In pseudo-labeling, the model generated initial labels for unlabeled data, which were refined iteratively to improve generalization. Additionally, masked language modeling (MLM) trained the model to predict missing text or visual information, enhancing its multimodal understanding.</p> <hr> <h4 id="6-technical-implementation"><strong>6. Technical Implementation</strong></h4> <p>The system was implemented with a combination of tools and frameworks:</p> <ul> <li> <p><strong>Multimodal Encoding:</strong> CLIP served as the primary multimodal encoder, aligning text and image data in a shared embedding space. Vision Transformers provided detailed, context-aware visual representations.</p> </li> <li> <p><strong>Cross-Attention Layers:</strong> Cross-attention layers created a feedback loop between visual and text data, enhancing the alignment of contexts. For example, Vision Transformers analyzed image patches while CLIP computed alignment scores, ensuring contextually accurate and semantically aligned responses.</p> </li> </ul> <p><strong>Distributed Training and Fine-Tuning</strong> The model used distributed training to handle large datasets and fine-tuning techniques to optimize performance on cross-modal tasks. Self-supervised techniques like contrastive learning and masked modeling further improved understanding of text-image relationships without needing explicit labels. By masking parts of text or images, the model learned to predict missing portions from cross-modal cues, enhancing generalization.</p> <hr> <h4 id="7-results-and-outcomes"><strong>7. Results and Outcomes</strong></h4> <p>This architecture showed notable improvements in Visual Question Answering and Image Captioning tasks:</p> <ul> <li> <p><strong>VQA Accuracy:</strong> Achieved a 35% improvement over baseline models, particularly in questions requiring complex visual-textual understanding.</p> </li> <li> <p><strong>Image Captioning Scores:</strong> Improved BLEU and METEOR scores by 30%, producing captions that were more accurate and descriptive.</p> </li> </ul> <p><strong>User Feedback and Impact</strong> Users reported that responses had greater depth and relevance, improving engagement and satisfaction. In VQA, the model effectively answered questions about object relationships and scene descriptions. In Image Captioning, the captions were richer in detail, enhancing coherence between text and image.</p> <hr> <h4 id="8-conclusion"><strong>8. Conclusion</strong></h4> <p>This project successfully addressed a major challenge in multimodal AI by combining CLIP, Vision Transformers, and cross-attention mechanisms for effective cross-modal understanding. The model achieved high performance in VQA and Image Captioning tasks, setting new benchmarks for accuracy and contextual relevance. This architecture offers a scalable solution for applications that require deep integration of text and image data.</p> <hr> <h4 id="9-skills-and-tools-used"><strong>9. Skills and Tools Used</strong></h4> <ul> <li> <strong>Core Technologies:</strong> CLIP, Vision Transformers</li> <li> <strong>Optimization Techniques:</strong> Cross-Attention Mechanisms, Contrastive Learning, Masked Language Modeling</li> <li> <strong>Data Handling:</strong> Pseudo-Labeling, Self-Training, Dual-Input Architecture</li> <li> <strong>Applications:</strong> Visual Question Answering, Image Captioning</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Sina Torfi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-start-here",title:"Start Here",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"During my career, I worked on many different projects. Below you see the selected ones. I created case studies to clarify the details for each. For the full list of projects, refer to my Project Portfolio.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"Repositories",description:"Top selected GitHub repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-leveraging-the-power-of-rag-the-next-step-in-information-retrieval",title:'Leveraging the Power of RAG: The Next Step in Information Retrieval <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/leveraging-the-power-of-rag-the-next-step-in-information-retrieval-458b15f28060?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-the-importance-of-deploying-machine-learning-models-a-comprehensive-analysis",title:'The Importance of Deploying Machine Learning Models: A Comprehensive Analysis <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/the-importance-of-deploying-machine-learning-models-a-comprehensive-analysis-3301db8d104d?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-unlock-the-power-of-openai-design-your-ml-pipeline-with-gpt-3-5-machine-learning-mindset",title:'Unlock the Power of OpenAI: Design Your ML Pipeline with GPT-3.5\u200a\u2014\u200aMachine Learning Mindset... <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/unlock-the-power-of-openai-design-your-ml-pipeline-with-gpt-3-5-machine-learning-mindset-9de8543a4efa?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-an-overview-of-machine-learning-system-design",title:'An Overview of Machine Learning System Design <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/an-overview-of-machine-learning-system-design-7ad5fe869eae?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-vision-transformers",title:'Vision Transformers <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/vision-transformers-1b80f052f1d2?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-the-easy-approach-to-access-a-kaggle-dataset-in-google-colab-machine-learning-mindset",title:'The Easy Approach to Access a Kaggle Dataset in Google Colab\u200a\u2014\u200aMachine Learning Mindset... <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/the-easy-approach-to-access-a-kaggle-dataset-in-google-colab-machine-learning-mindset-f04bf3c43b84?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-why-deep-learning-is-usually-the-number-1-trusted-choice",title:'Why Deep Learning is Usually The Number 1 Trusted Choice? <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/why-deep-learning-is-usually-the-number-1-trusted-choice-42776b8749ae?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-essential-definitions-in-probability-theory-that-you-need-to-know",title:'Essential Definitions in Probability Theory that You Need to Know <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/essential-definitions-in-probability-theory-that-you-need-to-know-8d330ceb007c?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-fascinating-neural-networks-an-ultra-short-introduction-machine-learning-mindset",title:'Fascinating Neural Networks: An Ultra Short Introduction\u200a\u2014\u200aMachine Learning Mindset <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/fascinating-neural-networks-an-ultra-short-introduction-machine-learning-mindset-9c8cc2b0146?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-probability-theory-and-its-huge-importance-in-machine-learning",title:'Probability Theory and its Huge Importance in Machine Learning <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/machine-learning-mindset/probability-theory-and-its-huge-importance-in-machine-learning-3a61b1601ccb?source=rss----7d2d5950b89b---4","_blank")}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-case-study",title:"Case Study",description:"Advanced AI-driven pathology analysis for cancer diagnostics for detection and localization of yumor cells.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-case-study",title:"Case Study",description:"Disease prediction with Adversarial AI, leveraging advanced Machine Learning techniques for enhanced diagnostic accuracy.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-case-study",title:"Case Study",description:"Enhanced Semantic Segmentation for AR/VR, by leveraging SWIN vision transformers, knowledge distillation, and domain adaptation.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-case-study",title:"Case Study",description:"Advanced Cross-Modal Knowledge Transfer for Enhanced Visual Question Answering and Image Captioning Using CLIP and Vision Transformers.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-case-study",title:"Case Study",description:"Enhanced Retrieval Augmented Generation System, Advanced Techniques for Improved Context and Accuracy in Conversational AI",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-case-study",title:"Case Study",description:"Synthetic Data Generation with Diffusion Models for Privacy-Preserving Healthcare Applications.",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-case-study",title:"Case Study",description:"Cross-Lingual Low-Resource Language Processing Using Self-Supervised Transformers.",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-case-study",title:"Case Study",description:"Advanced Recommender System Leveraging Deep Sequence Modeling and NLP for Enhanced User Personalization.",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2wkpsVwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/astorfi","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>