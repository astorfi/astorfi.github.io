<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Projects &mdash; Sina Torfi</title>
  <meta name="description" content="Research and applied AI projects by Sina Torfi — cancer detection, RAG systems, multimodal AI, and more.">
  <link rel="icon" type="image/svg+xml" href="assets/favicon.svg">
  <!-- Open Graph -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://astorfi.github.io/projects.html">
  <meta property="og:title" content="Projects — Sina Torfi">
  <meta property="og:description" content="Research and applied AI projects — RAG systems, cancer detection, multimodal AI, recommender systems, and more.">
  <meta property="og:image" content="https://astorfi.github.io/assets/img/prof_pic.png">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Projects — Sina Torfi">
  <meta name="twitter:description" content="Research and applied AI projects — RAG systems, cancer detection, multimodal AI, and more.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,400;0,6..72,600;0,6..72,700;1,6..72,400&family=Outfit:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="css/style.css">
</head>
<body>

<!-- Navbar -->
<nav class="navbar">
  <div class="container">
    <a class="nav-brand" href="index.html">Sina Torfi</a>
    <button class="nav-toggle" aria-label="Toggle menu">
      <span></span><span></span><span></span>
    </button>
    <ul class="nav-links">
      <li><a href="index.html">Home</a></li>
      <li><a href="experience.html">Experience</a></li>
      <li><a href="projects.html">Projects</a></li>
      <li><a href="publications.html">Publications</a></li>
      <li><a href="repositories.html">Repositories</a></li>
      <li><a href="cv.html">CV</a></li>
    </ul>
  </div>
</nav>

<!-- Page Header -->
<header class="page-header">
  <div class="container">
    <div class="section-label">Work</div>
    <h1>Projects</h1>
    <p>Selected research and applied AI projects. Click any card to explore the full case study.</p>
  </div>
</header>

<!-- Domain Filters + Projects -->
<section>
  <div class="container">

    <!-- Filter Bar -->
    <div class="filter-bar reveal" id="filterBar">
      <button class="filter-btn active" data-filter="all">All</button>
      <button class="filter-btn" data-filter="llms">LLMs</button>
      <button class="filter-btn" data-filter="nlp">NLP</button>
      <button class="filter-btn" data-filter="cv">Computer Vision</button>
      <button class="filter-btn" data-filter="genai">Generative AI</button>
      <button class="filter-btn" data-filter="healthcare">Healthcare</button>
      <button class="filter-btn" data-filter="multimodal">Multimodal</button>
      <button class="filter-btn" data-filter="recsys">Recommender Systems</button>
    </div>

    <!-- Project Cards Grid -->
    <div class="card-grid" id="projectGrid">

      <!-- 1. Enhanced RAG -->
      <div class="project-card reveal" data-domains="llms,nlp">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/enhanced_rag.png" alt="Enhanced RAG" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">LLMs</span>
                <span class="domain-pill">NLP</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Enhanced RAG</h3>
          <p>Advanced Retrieval-Augmented Generation system with multi-stage retrieval, re-ranking, and query decomposition for enterprise conversational AI.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Enterprise conversational AI systems struggle with hallucination, outdated knowledge, and inability to reason over large proprietary corpora in real time.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Multi-stage RAG-Fusion pipeline with parallel query generation and reciprocal rank fusion (RRF) for robust document retrieval</li>
                  <li>Dense retrieval with FAISS indexing over million-scale document collections</li>
                  <li>Contrastive learning with Momentum Contrast (MoCo) for domain-adaptive embedding fine-tuning</li>
                  <li>Domain-Adaptive Pretraining (DAPT) and in-context learning for specialized verticals</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>Significant improvement in factual grounding and answer relevance across enterprise benchmarks</li>
                  <li>Sub-second retrieval latency at million-document scale</li>
                  <li>Reduced hallucination rate through multi-hop evidence aggregation</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Core architecture powering the Amazon Q Chat Engine, serving millions of enterprise users with accurate, grounded conversational AI.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">LLMs</span>
              <span class="tech-pill">RAG-Fusion</span>
              <span class="tech-pill">FAISS</span>
              <span class="tech-pill">MoCo</span>
              <span class="tech-pill">DAPT</span>
              <span class="tech-pill">RRF</span>
              <span class="tech-pill">In-Context Learning</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 2. Agentic Function Calling & Tool Orchestration -->
      <div class="project-card reveal" data-domains="llms">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/agentic_function_calling.svg" alt="Agentic Function Calling" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">LLMs</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Agentic Function Calling &amp; Tool Orchestration</h3>
          <p>Multi-step agentic reasoning framework with structured function calling, tree-of-thought planning, and constrained decoding for enterprise tool orchestration.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Enterprise LLMs must decompose complex user requests into sequences of API calls, database queries, and tool invocations — maintaining coherence across multi-hop reasoning chains while handling errors gracefully and respecting authorization boundaries.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Structured function-calling framework with JSON schema validation and type-safe tool definitions supporting parallel and sequential execution</li>
                  <li>ReAct-style iterative reasoning with tree-of-thought planning for multi-step task decomposition and dependency graph construction</li>
                  <li>Constrained decoding via grammar-guided generation (GCD) for guaranteed schema adherence and valid structured outputs</li>
                  <li>Fine-tuned function-calling models using synthetic trajectory generation and DPO on execution feedback signals — learning from whether tool calls succeed, not just human preference</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>Significant improvement in multi-step task completion rate over chain-of-thought baselines</li>
                  <li>Near-perfect schema adherence via constrained generation, eliminating malformed tool calls</li>
                  <li>Robust error recovery and autonomous re-planning when tool calls fail mid-chain</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Core orchestration layer for enterprise agentic AI, enabling complex multi-tool workflows with reliable structured outputs across diverse enterprise knowledge sources and APIs.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">ReAct</span>
              <span class="tech-pill">Tree-of-Thought</span>
              <span class="tech-pill">Constrained Decoding</span>
              <span class="tech-pill">DPO</span>
              <span class="tech-pill">Function Calling</span>
              <span class="tech-pill">JSON Schema</span>
              <span class="tech-pill">Trajectory Synthesis</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 3. LLM Post-Training Alignment & Safety -->
      <div class="project-card reveal" data-domains="llms,genai">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/llm_alignment_safety.svg" alt="LLM Alignment & Safety" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">LLMs</span>
                <span class="domain-pill">Generative AI</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>LLM Post-Training Alignment &amp; Safety</h3>
          <p>End-to-end alignment pipeline combining DPO/KTO preference optimization, constitutional AI guardrails, automated red-teaming, and PII-aware decoding for enterprise LLM deployment.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Deploying LLMs in enterprise environments demands rigorous alignment — balancing helpfulness with safety, preventing PII leakage, mitigating hallucination, and ensuring compliance with enterprise policies — where a single violation can have serious consequences.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Multi-stage post-training pipeline: supervised fine-tuning &rarr; DPO/KTO preference optimization &rarr; constitutional AI filtering, eliminating the reward model bottleneck of classical RLHF</li>
                  <li>Automated red-teaming infrastructure with adversarial prompt generation, gradient-based attack simulation, and jailbreak taxonomy coverage analysis</li>
                  <li>PII-aware decoding with named entity recognition guardrails and real-time output sanitization layers</li>
                  <li>Reward model ensembles with uncertainty quantification for calibrated refusal — the model knows when it doesn't know</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>Significant reduction in policy violations while maintaining task performance on enterprise benchmarks</li>
                  <li>Near-zero PII leakage rate across enterprise deployment surfaces</li>
                  <li>Scalable red-teaming pipeline generating diverse adversarial test cases at thousands per hour</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Safety and alignment infrastructure for trusted enterprise LLM deployment, ensuring compliant and reliable AI interactions at scale across regulated industries.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">DPO</span>
              <span class="tech-pill">KTO</span>
              <span class="tech-pill">Constitutional AI</span>
              <span class="tech-pill">Red-Teaming</span>
              <span class="tech-pill">PII Detection</span>
              <span class="tech-pill">Reward Modeling</span>
              <span class="tech-pill">Safety Classifiers</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 4. Efficient LLM Serving & Speculative Decoding -->
      <div class="project-card reveal" data-domains="llms">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/efficient_llm_serving.svg" alt="Efficient LLM Serving" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">LLMs</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Efficient LLM Serving &amp; Speculative Decoding</h3>
          <p>Production LLM serving stack combining speculative decoding, KV-cache compression, quantization-aware fine-tuning, and continuous batching for enterprise-scale inference.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Serving large language models at enterprise scale demands sub-second latency, high throughput, and cost efficiency — while preserving output quality across diverse workloads with millions of concurrent users.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Speculative decoding with a distilled draft model — small model proposes token sequences, large model verifies in parallel, achieving 2-3x throughput with mathematically identical outputs</li>
                  <li>KV-cache compression with grouped-query attention (GQA) and sliding window strategies for long-context serving without linear memory growth</li>
                  <li>Quantization-aware fine-tuning (AWQ/GPTQ) with task-specific calibration sets for INT4 deployment with minimal quality degradation</li>
                  <li>Dynamic continuous batching with PagedAttention for GPU memory management, maximizing hardware utilization across heterogeneous request lengths</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>2.5x inference throughput improvement via speculative decoding at matched output quality</li>
                  <li>60% memory reduction through KV-cache compression, enabling 4x longer context windows</li>
                  <li>Production INT4 quantization with &lt;1% quality degradation on enterprise benchmarks</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Powers high-throughput, low-latency LLM serving infrastructure for millions of concurrent enterprise users, reducing inference cost while maintaining quality guarantees.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">Speculative Decoding</span>
              <span class="tech-pill">AWQ</span>
              <span class="tech-pill">GPTQ</span>
              <span class="tech-pill">PagedAttention</span>
              <span class="tech-pill">KV-Cache</span>
              <span class="tech-pill">GQA</span>
              <span class="tech-pill">Continuous Batching</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 5. Semantic Segmentation for AR/VR -->
      <div class="project-card reveal" data-domains="cv,multimodal">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/eye_semantic_segmentation.png" alt="Semantic Segmentation" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">Computer Vision</span>
                <span class="domain-pill">Multimodal</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Semantic Segmentation for AR/VR</h3>
          <p>State-of-the-art semantic segmentation for AR/VR eye tracking using SWIN Vision Transformers, achieving 0.96 mIoU with on-device deployment.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>AR/VR devices require pixel-precise eye region segmentation at real-time speeds on power-constrained hardware, with robustness to extreme lighting and motion.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>SWIN Vision Transformer backbone with UperNet decoder for multi-scale feature fusion</li>
                  <li>Knowledge distillation from large teacher to compact student model</li>
                  <li>Quantization-aware training (QAT) for INT8 on-device inference</li>
                  <li>Adversarial domain adaptation with diffusion-based augmentation for distribution shift</li>
                  <li>Active learning pipeline with weak supervision for efficient annotation</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>0.96 mIoU on internal AR/VR eye segmentation benchmark</li>
                  <li>4x inference speedup through distillation + quantization</li>
                  <li>Robust to cross-device and cross-user distribution shifts</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Deployed in Meta Reality Labs AR/VR pipeline, enabling precise gaze tracking and foveated rendering for next-generation headsets.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">SWIN Transformer</span>
              <span class="tech-pill">UperNet</span>
              <span class="tech-pill">Knowledge Distillation</span>
              <span class="tech-pill">QAT</span>
              <span class="tech-pill">Domain Adaptation</span>
              <span class="tech-pill">Active Learning</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 3. MultiModal Knowledge Transfer -->
      <div class="project-card reveal" data-domains="cv,nlp,multimodal">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/multi_modal_clip.png" alt="MultiModal Knowledge Transfer" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">Multimodal</span>
                <span class="domain-pill">CV</span>
                <span class="domain-pill">NLP</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>MultiModal Knowledge Transfer</h3>
          <p>Cross-modal knowledge transfer framework using CLIP and Vision Transformers for zero-shot Visual Question Answering and Image Captioning.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Bridging vision and language modalities for VQA and captioning requires massive paired datasets. Enabling zero-shot transfer to new domains without task-specific fine-tuning remains an open problem.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>CLIP-based vision-language alignment with cross-attention fusion layers</li>
                  <li>Contrastive learning objectives for joint embedding space optimization</li>
                  <li>Masked language modeling + pseudo-labeling for self-training on unlabeled data</li>
                  <li>Distributed training across multi-GPU clusters for billion-parameter models</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>Zero-shot generalization to unseen visual domains and question types</li>
                  <li>Competitive with supervised baselines using 10x less labeled data</li>
                  <li>Scalable to billion-parameter models with linear training efficiency</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Framework adopted across Meta product surfaces for visual understanding tasks, reducing annotation cost and enabling rapid domain expansion.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">CLIP</span>
              <span class="tech-pill">Vision Transformers</span>
              <span class="tech-pill">Cross-Attention</span>
              <span class="tech-pill">Contrastive Learning</span>
              <span class="tech-pill">Pseudo-Labeling</span>
              <span class="tech-pill">Distributed Training</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 4. Low-Resource LLMs -->
      <div class="project-card reveal" data-domains="llms,nlp">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/low_resource_llm.png" alt="Low-Resource LLMs" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">LLMs</span>
                <span class="domain-pill">NLP</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Low-Resource LLMs</h3>
          <p>Cross-lingual language understanding using self-supervised transformers for underrepresented languages with minimal labeled data.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Most NLP advances concentrate on high-resource languages. Extending LLM capabilities to hundreds of low-resource languages requires novel transfer learning and data augmentation strategies.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Multilingual pre-training with mBERT and XLM-R on cross-lingual corpora</li>
                  <li>Cross-lingual alignment via adversarial training on shared embedding spaces</li>
                  <li>Back-translation augmentation to synthetically expand low-resource training data</li>
                  <li>Masked language modeling (MLM) fine-tuning with language-adaptive layers</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>Strong performance gains on NER, classification, and QA tasks for low-resource languages</li>
                  <li>Effective zero-shot cross-lingual transfer from high-resource to unseen languages</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Enables language understanding for underrepresented populations, supporting equitable AI deployment across global markets.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">mBERT</span>
              <span class="tech-pill">XLM-R</span>
              <span class="tech-pill">Cross-Lingual Transfer</span>
              <span class="tech-pill">Adversarial Training</span>
              <span class="tech-pill">Back-Translation</span>
              <span class="tech-pill">MLM</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 5. Cancer Detection -->
      <div class="project-card reveal" data-domains="cv,healthcare">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/cancer_detection.png" alt="Cancer Detection" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">Computer Vision</span>
                <span class="domain-pill">Healthcare</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Cancer Detection</h3>
          <p>AI-driven pathology analysis system for tumor detection and localization using hierarchical CNNs and self-supervised learning on whole-slide images.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Manual pathology review is slow and error-prone. Whole-slide images are gigapixel-scale, requiring architectures that handle extreme resolution while maintaining fine-grained localization.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Hierarchical CNN with attention mechanisms for multi-scale feature extraction from gigapixel slides</li>
                  <li>Self-supervised pre-training on unlabeled pathology images to learn robust histological representations</li>
                  <li>Transfer learning from ImageNet with progressive fine-tuning</li>
                  <li>End-to-end deployment on AWS SageMaker with Lambda-based inference API</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>High sensitivity and specificity for tumor detection on clinical datasets</li>
                  <li>Precise tumor localization with attention-guided heatmaps</li>
                  <li>Scalable to clinical throughput on cloud infrastructure</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Accelerates pathology workflows and provides decision support for clinicians, reducing diagnostic turnaround time.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">Hierarchical CNN</span>
              <span class="tech-pill">Self-Supervised Learning</span>
              <span class="tech-pill">Attention Mechanisms</span>
              <span class="tech-pill">AWS SageMaker</span>
              <span class="tech-pill">Transfer Learning</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 6. Disease Prediction -->
      <div class="project-card reveal" data-domains="healthcare,genai">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/disease_prediction.png" alt="Disease Prediction" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">Healthcare</span>
                <span class="domain-pill">Generative AI</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Disease Prediction</h3>
          <p>Adversarial AI and causal inference framework for unbiased disease prediction, combining GANs with Double ML for robust diagnostic models.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Clinical prediction models inherit biases from training data — demographic, socioeconomic, and selection biases — leading to disparate outcomes across patient populations.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Adversarial learning to decorrelate predictions from sensitive attributes</li>
                  <li>Double ML framework for causal effect estimation under confounding</li>
                  <li>VQ-VAE and GAN-based synthetic data augmentation for minority groups</li>
                  <li>Clustering-based patient stratification for personalized risk scoring</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>Measurable reduction in prediction disparity across demographic groups</li>
                  <li>Maintained clinical accuracy while improving fairness metrics</li>
                  <li>Robust causal estimates under confounding scenarios</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Advances equitable healthcare AI by ensuring diagnostic models perform fairly across all patient populations.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">Adversarial Learning</span>
              <span class="tech-pill">Double ML</span>
              <span class="tech-pill">VQ-VAE</span>
              <span class="tech-pill">GANs</span>
              <span class="tech-pill">Causal Inference</span>
              <span class="tech-pill">Patient Stratification</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 7. Synthetic Data Generation -->
      <div class="project-card reveal" data-domains="genai,healthcare">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/privacy_preserving_ai.png" alt="Synthetic Data Generation" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">Generative AI</span>
                <span class="domain-pill">Healthcare</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Synthetic Data Generation</h3>
          <p>Privacy-preserving synthetic medical data using diffusion models and convolutional GANs with formal differential privacy guarantees.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>Healthcare AI research is bottlenecked by data access — patient privacy regulations (HIPAA) prevent sharing real medical records, limiting model development and reproducibility.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Convolutional GAN architecture with differential privacy (DP-SGD) for formal privacy guarantees</li>
                  <li>Diffusion model pipelines with controlled noise scheduling for high-fidelity generation</li>
                  <li>Privacy auditing via membership inference attacks to validate protection</li>
                  <li>Statistical fidelity metrics ensuring synthetic data preserves clinical distributions</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>Synthetic data passes privacy audits while maintaining downstream model utility</li>
                  <li>Published in Information Sciences (140+ citations)</li>
                  <li>Enables HIPAA-compliant data sharing for multi-site research</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Unlocks healthcare AI research by providing shareable, privacy-safe synthetic datasets — cited 140+ times and adopted by research groups globally.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">Diffusion Models</span>
              <span class="tech-pill">DP-SGD</span>
              <span class="tech-pill">Convolutional GANs</span>
              <span class="tech-pill">Privacy Auditing</span>
              <span class="tech-pill">FAISS</span>
              <span class="tech-pill">U-Net</span>
            </div>
          </div>
        </div>
      </div>

      <!-- 8. Deep Sequence Recommender -->
      <div class="project-card reveal" data-domains="nlp,recsys">
        <div class="project-card-header">
          <div class="project-thumb-wrap">
            <img src="assets/img/projects/recommender_system_dsm.png" alt="Deep Sequence Recommender" loading="lazy">
            <div class="project-thumb-overlay">
              <div class="domain-pills">
                <span class="domain-pill">Recommender Systems</span>
                <span class="domain-pill">NLP</span>
              </div>
              <div class="expand-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div>
            </div>
          </div>
        </div>
        <div class="project-summary">
          <h3>Deep Sequence Recommender</h3>
          <p>Production recommender system using Transformer-XL and meta-learning for temporal-aware personalization at billion-user scale.</p>
        </div>
        <div class="project-detail">
          <div class="project-detail-inner">
            <div class="detail-grid">
              <div class="detail-block">
                <h4>Challenge</h4>
                <p>User preferences evolve over time and new users lack interaction history. Traditional collaborative filtering fails to capture temporal dynamics and suffers from cold-start problems.</p>
              </div>
              <div class="detail-block">
                <h4>Approach</h4>
                <ul>
                  <li>Transformer-XL architecture for long-range sequential dependency modeling</li>
                  <li>Model-Agnostic Meta-Learning (MAML) for few-shot cold-start user adaptation</li>
                  <li>NLP-enriched item representations using BERT and contextual embeddings</li>
                  <li>Production deployment with ONNX/TensorRT optimization and Kubernetes orchestration</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Results</h4>
                <ul>
                  <li>Improvement in recommendation relevance metrics over production baseline</li>
                  <li>Effective cold-start handling with meta-learned user priors</li>
                  <li>Sub-100ms inference latency at billion-scale with optimized serving</li>
                </ul>
              </div>
              <div class="detail-block">
                <h4>Impact</h4>
                <p>Serving billions of daily predictions in Meta's recommendation surfaces, directly impacting user engagement and content discovery.</p>
              </div>
            </div>
            <div class="tech-pills">
              <span class="tech-pill">Transformer-XL</span>
              <span class="tech-pill">MAML</span>
              <span class="tech-pill">BERT</span>
              <span class="tech-pill">ONNX</span>
              <span class="tech-pill">TensorRT</span>
              <span class="tech-pill">Kubernetes</span>
            </div>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="footer-inner">
      <div class="footer-copy">&copy; 2026 Sina Torfi. San Jose, CA.</div>
      <div class="footer-links">
        <a href="https://github.com/astorfi" target="_blank" rel="noopener" aria-label="GitHub">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
        </a>
        <a href="https://www.linkedin.com/in/sinalk" target="_blank" rel="noopener" aria-label="LinkedIn">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
        </a>
        <a href="https://scholar.google.com/citations?user=2wkpsVwAAAAJ&hl=en" target="_blank" rel="noopener" aria-label="Google Scholar">
          <svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"/></svg>
        </a>
      </div>
    </div>
  </div>
</footer>

<script src="js/main.js"></script>
<script src="js/projects.js"></script>
</body>
</html>
